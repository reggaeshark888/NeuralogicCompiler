{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchviz import make_dot\n",
    "\n",
    "def print_network_parameters(network):\n",
    "    # Print weights and biases\n",
    "    for name, param in network.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"{name}:\")\n",
    "            print(param.data)\n",
    "\n",
    "def create_torch_XOR_dataset():\n",
    "    x1 = np.array ([0., 0., 1., 1.], dtype = np.float32)\n",
    "    x2 = np.array ([0., 1., 0., 1.], dtype = np.float32)\n",
    "    y  = np.array ([0., 1., 1., 0.],dtype = np.float32)\n",
    "\n",
    "    x1 = np.repeat(x1, 50)\n",
    "    x2 = np.repeat(x2, 50)  \n",
    "    y =  np.repeat(y,  50)\n",
    "    \n",
    "    # Add noise\n",
    "    x1 = x1 + np.random.rand(x1.shape[0])*0.05\n",
    "    x2 = x2 + np.random.rand(x2.shape[0])*0.05\n",
    "\n",
    "    # Shuffle the data\n",
    "    index_shuffle = np.arange(x1.shape[0])\n",
    "    np.random.shuffle(index_shuffle)\n",
    "\n",
    "    x1 = x1.astype(np.float32)\n",
    "    x2 = x2.astype(np.float32)\n",
    "    y = y.astype(np.float32)\n",
    "\n",
    "    x1 = x1[index_shuffle]\n",
    "    x2 = x2[index_shuffle]\n",
    "    y = y[index_shuffle]\n",
    "\n",
    "    # Convert data to pytorch tensors\n",
    "    x1_torch = torch.from_numpy(x1).clone().view(-1,1)\n",
    "    x2_torch = torch.from_numpy(x2).clone().view(-1,1)\n",
    "    y_torch = torch.from_numpy(y).clone().view(-1,1)\n",
    "\n",
    "    X = torch.hstack([x1_torch, x2_torch])\n",
    "\n",
    "    X_train = X[:150,:]\n",
    "    X_test  = X[150:,:]\n",
    "    y_train = y_torch[:150,:]\n",
    "    y_test  = y_torch[150:,:]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def train(model, loss_function, optimizer, x, y, no_of_epochs):\n",
    "    # store loss for each epoch\n",
    "    all_loss = []\n",
    "\n",
    "    for epoch_index in range(no_of_epochs):\n",
    "        # forward pass\n",
    "        y_hat = model(x)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = loss_function(y_hat, y)\n",
    "        all_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "\n",
    "        # optimize the weights and bias\n",
    "\n",
    "        # takes a step in the parameter step opposite to the gradient, peforms the update rule\n",
    "        optimizer.step()\n",
    "\n",
    "        # clears out the old gradients from the previous step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        print(all_loss[epoch_index])\n",
    "\n",
    "    return all_loss\n",
    "\n",
    "def train_with_rectified_L2(model, loss_function, optimizer, x, y, no_of_epochs):\n",
    "    all_loss = []\n",
    "\n",
    "    for epoch_index in range(no_of_epochs):\n",
    "        y_hat = model(x)\n",
    "        # Regular loss calculation\n",
    "        loss = loss_function(y_hat, y)\n",
    "\n",
    "        \n",
    "\n",
    "class TwoLayerMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TwoLayerMLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(2, 2)\n",
    "        self.output_layer = nn.Linear(2, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.layer1(x))\n",
    "        x = torch.sigmoid(self.output_layer(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_torch_XOR_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_train, y_train, X_test, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_torch_XOR_dataset\u001b[49m()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# plt.scatter(X_train[:,0], X_train[:,1], c = y_train)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model_XOR \u001b[38;5;241m=\u001b[39m TwoLayerMLP()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_torch_XOR_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = create_torch_XOR_dataset()\n",
    "# plt.scatter(X_train[:,0], X_train[:,1], c = y_train)\n",
    "# plt.show()\n",
    "\n",
    "model_XOR = TwoLayerMLP()\n",
    "\n",
    "# define the loss\n",
    "loss_function = torch.nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model_XOR.parameters(), lr=0.1)\n",
    "all_loss = train(model_XOR, loss_function, optimizer, X_train, y_train, 90000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output0-0: tensor([[0.0010]], grad_fn=<SigmoidBackward0>)\n",
      "Output0-1: tensor([[0.9989]], grad_fn=<SigmoidBackward0>)\n",
      "Output1-0: tensor([[0.9986]], grad_fn=<SigmoidBackward0>)\n",
      "Output1-1: tensor([[0.0009]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example input: a batch of 1 sample with 2 features\n",
    "input1 = torch.tensor([[0.0, 0.0]])\n",
    "input2 = torch.tensor([[0.0, 1.0]])\n",
    "input3 = torch.tensor([[1.0, 0.0]])\n",
    "input4 = torch.tensor([[1.0, 1.0]])\n",
    "# Forward pass to get the output\n",
    "output1 = model_XOR(input1)\n",
    "output2 = model_XOR(input2)\n",
    "output3 = model_XOR(input3)\n",
    "output4 = model_XOR(input4)\n",
    "\n",
    "print(\"Output0-0:\", output1)\n",
    "print(\"Output0-1:\", output2)\n",
    "print(\"Output1-0:\", output3)\n",
    "print(\"Output1-1:\", output4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1.weight:\n",
      "  Neuron 1 weights: [-7.3517265  7.0466037]\n",
      "  Neuron 2 weights: [-7.790935   7.9490924]\n",
      "layer1.bias:\n",
      "  Neuron 1 bias: -3.7243492603302\n",
      "  Neuron 2 bias: 3.8839123249053955\n",
      "output_layer.weight:\n",
      "  Neuron 1 weights: [ 14.864945 -14.461322]\n",
      "output_layer.bias:\n",
      "  Neuron 1 bias: 6.887348175048828\n"
     ]
    }
   ],
   "source": [
    "def print_network_parameters_for_neurons(network):\n",
    "    # Print weights and biases for each neuron\n",
    "    for name, param in network.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"{name}:\")\n",
    "            if name.endswith('weight'):  # If the parameter is a weight\n",
    "                for idx, weights in enumerate(param):\n",
    "                    print(f\"  Neuron {idx + 1} weights: {weights.data.numpy()}\")\n",
    "            elif name.endswith('bias'):  # If the parameter is a bias\n",
    "                for idx, bias in enumerate(param):\n",
    "                    print(f\"  Neuron {idx + 1} bias: {bias.data.item()}\")\n",
    "\n",
    "\n",
    "print_network_parameters_for_neurons(model_XOR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
